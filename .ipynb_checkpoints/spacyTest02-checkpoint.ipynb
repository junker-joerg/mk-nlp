{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de')\n",
    "doc = nlp(u'Ich bin ein Berliner.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = (u'Der Apfel und die Orange sind ähnlich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjkoe\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.24070129531972922"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_apfel = doc[:2]\n",
    "die_orange = doc[3:5]\n",
    "der_apfel.similarity(die_orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Den Berliner hat der Hund nicht gebissen.')\n",
    "# heads array: [1, 6, 2, 4, 2, 6, 2, 2] (second token is attached with a non-projective arc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den/DET Berliner/NOUN hat/AUX der/DET Hund/NOUN nicht/PART gebissen/VERB ./PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.pos_) for t in doc))\n",
    "# output: Ich/PRON bin/AUX ein/DET Berliner/NOUN ./PUNCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den      <--nk--- Berliner\n",
      "Berliner <--oa--- gebissen\n",
      "hat      <-ROOT-- hat\n",
      "der      <--nk--- Hund\n",
      "Hund     <--sb--- hat\n",
      "nicht    <--ng--- gebissen\n",
      "gebissen <--oc--- hat\n",
      ".        <-punct- hat\n"
     ]
    }
   ],
   "source": [
    "# show dependency arcs\n",
    "print('\\n'.join('{child:<8} <{label:-^7} {head}'.format(child=t.orth_, label=t.dep_, head=t.head.orth_) for t in doc))\n",
    "# output: (sb: subject, nk: noun kernel, pd: predicate)\n",
    "# Ich      <--sb--- bin\n",
    "# bin      <-ROOT-- bin\n",
    "# ein      <--nk--- Berliner\n",
    "# Berliner <--pd--- bin\n",
    "# .        <-punct- bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berliner\n"
     ]
    }
   ],
   "source": [
    "# show named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)\n",
    "# output:\n",
    "# Berline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den Berliner\n",
      "der Hund\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'noun_chunks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8bb1a0a94a9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# noun chunks include so-called measure constructions ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mu'Ich möchte gern zum Essen eine Tasse Kaffee bestellen.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# output:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# [Essen, eine Tasse Kaffee]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'noun_chunks'"
     ]
    }
   ],
   "source": [
    "# show noun chunks\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)\n",
    "# output:\n",
    "# ein Berliner\n",
    "\n",
    "# noun chunks include so-called measure constructions ...\n",
    "doc = (u'Ich möchte gern zum Essen eine Tasse Kaffee bestellen.')\n",
    "print( [ chunk for chunk in doc.noun_chunks ])\n",
    "# output:\n",
    "# [Essen, eine Tasse Kaffee]\n",
    "\n",
    "# ... and close appositions\n",
    "doc = (u'Der Senator vermeidet das Thema Flughafen.')\n",
    "print( [ chunk for chunk in doc.noun_chunks ])\n",
    "# output:\n",
    "# [Der Senator, das Thema Flughafen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1306665"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use word vectors\n",
    "de = spacy.load('de')\n",
    "doc = de(u'Der Apfel und die Orange sind ähnlich')\n",
    "assert len(doc.vector) == len(doc[0].vector)\n",
    "der_apfel = doc[:2]\n",
    "die_orange = doc[3:5]\n",
    "der_apfel.similarity(die_orange)\n",
    "# output:\n",
    "# 0.63665210991205579\n",
    "der, apfel = der_apfel\n",
    "der.similarity(apfel)\n",
    "# output:\n",
    "# 0.24995991403916812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2 und\n"
     ]
    }
   ],
   "source": [
    "# the root has no left dependents:\n",
    "print(doc[2].n_lefts)\n",
    "# output:\n",
    "# 0\n",
    "\n",
    "# but the root's left-most descendant is not the root itself but a token further left\n",
    "print(doc[2].left_edge.i, doc[2].left_edge.orth_)\n",
    "# output:\n",
    "# (0, u'Den')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
